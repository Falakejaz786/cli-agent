{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97720425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"commandline_qa.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "questions = [item[\"question\"] for item in data]\n",
    "answers = [item[\"answer\"] for item in data]\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"question\": questions, \"answer\": answers})\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6997c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e10782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 168/168 [00:00<00:00, 347.77 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 238.26 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 238.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_length = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    prompts = [f\"Question: {q}\\nAnswer:\" for q in batch[\"question\"]]\n",
    "    inputs = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    outputs = tokenizer(batch[\"answer\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cd0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 125,493,504 || trainable%: 0.2350\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1101f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT0001\\AppData\\Local\\Temp\\ipykernel_27628\\4074003995.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-gptneo\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,  \n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e1e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " I have a script that checks if a variable is set in Bash.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the -f option to check if the variable is set.\n",
      "$ echo \"variable set\" | grep -q \"variable set\"\n",
      "variable set\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the -f option to check if the variable is set.\n",
      "$ echo \"variable set\" | grep -q \"variable set\"\n",
      "variable set\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the -f option to check if the variable is set.\n",
      "$ echo \"variable set\" | grep -q \"variable\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How do I check if a variable is set in Bash?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\", 1).strip()\n",
    "print(\"Generated Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a934921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at ['./lora-gptneo/checkpoint-90', 'lora-gptneo/checkpoint-90', 'c:\\\\Users\\\\KIIT0001\\\\Desktop\\\\StackoverflowQ&A\\\\lora-gptneo\\\\checkpoint-90']; using the current model\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: How to concatenate string variables in Bash\n",
      "Reference:\n",
      " foo=\"Hello\"\n",
      "foo=\"${foo} World\"\n",
      "echo \"${foo}\"\n",
      "> Hello World\n",
      "\n",
      "\n",
      "In general to concatenate two variables you can just write them one after another:\n",
      "\n",
      "a='Hello'\n",
      "b='World'\n",
      "c=\"${a} ${b}\"\n",
      "echo \"${c}\"\n",
      "> Hello World\n",
      "Generated:\n",
      " A:\n",
      "\n",
      "You can do it like this:\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "echo $1\n",
      "echo $2\n",
      "echo $3\n",
      "echo $4\n",
      "echo $5\n",
      "echo $6\n",
      "echo $7\n",
      "echo $8\n",
      "echo $9\n",
      "echo $10\n",
      "echo $11\n",
      "echo $12\n",
      "echo $13\n",
      "echo $14\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: How do I iterate over a range of numbers defined by variables in Bash?\n",
      "Reference:\n",
      " for i in $(seq 1 $END); do echo $i; done\n",
      "\n",
      "edit: I prefer seq over the other methods because I can actually remember it ;)\n",
      "Generated:\n",
      " If you want to iterate over a range of numbers defined by variables in Bash, you can do it like this:\n",
      "for i in range(1,10):\n",
      "    for j in range(1,10):\n",
      "        print(i,j)\n",
      "\n",
      "A:\n",
      "\n",
      "If you want to iterate over a range of numbers defined by variables in Bash, you can do it like this:\n",
      "for i in range(1,10):\n",
      "    for j in range(1,10):\n",
      "        print(i,j)\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Git is not working after macOS update (&quot;xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools&quot;)\n",
      "Reference:\n",
      " The problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n",
      "\n",
      "Did not run into this on Sonoma.\n",
      "\n",
      "Maybe Apple fixed the process?\n",
      "\n",
      "\n",
      "Updated for Ventura\n",
      "\n",
      "After opening the terminal after restarting, I tried to go to my code, and do a git status, and I got an error and prompt for command line software agreement:\n",
      "So press space until you get to the [agree, print, cancel] option, so careful hit space to scroll down to the end, if you blow past It you have to run a command to get it back. Use sudo xcodebuild -license to get to it again.\n",
      "Just be careful on scrolling down and enter agree and press return and it will launch into an update.\n",
      "\n",
      "Then I tried to use git after the install, and it prompted me to install Xcode tools again.\n",
      "I followed my own advice from previous years (see below), and went to https://developer.apple.com/download/all and downloaded\n",
      "\"Command Line Tools for Xcode 14\" (You have to log in with your Apple ID and enter MFA code, so have all the devices you need for that handy. Then select \"Command Line Tools for Xcode 14\", or if you want to get into the alphas or betas, that's up to you. But stable releases are probably the best choice for software developers.\n",
      "\n",
      "You have to either download the tools from CLI or the developer page and before you can use git, you need to reboot!!! Or you will get stuck in a loop of prompt & downloading\n",
      "Rebooting will break the loop and complete the installation of your CLI tools including git so that you can get back to work\n",
      "Solutions for previous years, these may or may not be valid these days as the downloads page has changed significantly:\n",
      "PREVIOUS YEARS SOLUTIONS, probably #2 is most helpful.\n",
      "*** Solution #1:\n",
      "Go back to your terminal and enter:\n",
      "xcode-select --install\n",
      "\n",
      "You'll then receive the following output:\n",
      "xcode-select: note: install requested for command line developer tools\n",
      "\n",
      "You will then be prompted in a window to update Xcode Command Line tools. (which could take a while)\n",
      "Open a new terminal window and your development tools should be returned.\n",
      "Addition: With any major or semi-major update you'll need to update the command line tools in order to get them functioning properly again. Check Xcode with any update. This goes beyond Mojave...\n",
      "After that restart your terminal\n",
      "Alternatively, IF that fails, and it might.... you'll get a pop-up box saying \"Software not found on server\", proceed to solution 2.\n",
      "*** Solution #2: (Preferred method)\n",
      "If you hit xcode-select --install and it doesn't find the software, log into Apple Developer, and install it via webpage.\n",
      "Log in or sign up here:\n",
      "https://developer.apple.com/download/more/\n",
      "Look for: \"Command Line Tools for Xcode 14.x\" in the list of downloads\n",
      "Then click the dmg and download. (See previous image above) either way, you will probably wind up at an apple downloads webpage.\n",
      "Generated:\n",
      " It's not working after macOS update.\n",
      "\n",
      "A:\n",
      "\n",
      "The problem is that you are trying to run a command that is not in the developer path.\n",
      "You can try to run a command that is in the developer path:\n",
      "git remote add origin git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github\n",
      "\n",
      "--- Example 4 ---\n",
      "Question: Defining a variable with or without export\n",
      "Reference:\n",
      " export makes the variable available to sub-processes.\n",
      "That is,\n",
      "export name=value\n",
      "\n",
      "means that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\n",
      "name=value\n",
      "\n",
      "means the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc. An important exception to this rule is that if you define the variable while running a command, that variable will be available to child processes. For example\n",
      "MY_VAR=yay node my-script.js\n",
      "\n",
      "In this case MY_VAR will be available to the node process running my-script.\n",
      "It's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it.\n",
      "Generated:\n",
      " Defining a variable with or without export\n",
      "\n",
      "A:\n",
      "\n",
      "You can define a variable with or without export.\n",
      "\n",
      "A:\n",
      "\n",
      "You can define a variable with or without export.\n",
      "\n",
      "A:\n",
      "\n",
      "You can define a variable with or without export.\n",
      "\n",
      "A:\n",
      "\n",
      "You can define a variable with or without export.\n",
      "\n",
      "You can define a variable with or without export.\n",
      "\n",
      "--- Example 5 ---\n",
      "Question: How to convert a string to lower case in Bash\n",
      "Reference:\n",
      " There are various ways:\n",
      "POSIX standard\n",
      "tr\n",
      "$ echo \"$a\" | tr '[:upper:]' '[:lower:]'\n",
      "hi all\n",
      "\n",
      "AWK\n",
      "$ echo \"$a\" | awk '{print tolower($0)}'\n",
      "hi all\n",
      "\n",
      "Non-POSIX\n",
      "You may run into portability issues with the following examples:\n",
      "Bash 4.0\n",
      "$ echo \"${a,,}\"\n",
      "hi all\n",
      "\n",
      "sed\n",
      "$ echo \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\n",
      "hi all\n",
      "# this also works:\n",
      "$ sed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\n",
      "hi all\n",
      "\n",
      "Perl\n",
      "$ echo \"$a\" | perl -ne 'print lc'\n",
      "hi all\n",
      "\n",
      "Bash\n",
      "lc(){\n",
      "    case \"$1\" in\n",
      "        [A-Z])\n",
      "        n=$(printf \"%d\" \"'$1\")\n",
      "        n=$((n+32))\n",
      "        printf \\\\$(printf \"%o\" \"$n\")\n",
      "        ;;\n",
      "        *)\n",
      "        printf \"%s\" \"$1\"\n",
      "        ;;\n",
      "    esac\n",
      "}\n",
      "word=\"I Love Bash\"\n",
      "for((i=0;i<${#word};i++))\n",
      "do\n",
      "    ch=\"${word:$i:1}\"\n",
      "    lc \"$ch\"\n",
      "done\n",
      "\n",
      "Note: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch  is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)\n",
      "Generated:\n",
      " $ echo \"hello world\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\" | sed -e \"s/hello/g\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from peft import PeftModel\n",
    "\n",
    "ckpt_dir = \"./lora-gptneo/checkpoint-90\"\n",
    "possible_paths = [ckpt_dir, \"lora-gptneo/checkpoint-90\", os.path.join(os.getcwd(), \"lora-gptneo\", \"checkpoint-90\")]\n",
    "found_path = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p):\n",
    "        found_path = p\n",
    "        break\n",
    "\n",
    "if found_path:\n",
    "    try:\n",
    "        loaded_model = PeftModel.from_pretrained(base_model, found_path)\n",
    "        loaded_model = loaded_model.to(device)\n",
    "        print(f\"Loaded LoRA adapter from {found_path} and moved to {device}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load LoRA adapter from checkpoint:\", e)\n",
    "        loaded_model = model  \n",
    "else:\n",
    "    print(f\"No checkpoint found at {possible_paths}; using the current model\")\n",
    "    loaded_model = model\n",
    "\n",
    "num_examples = 5\n",
    "if len(dataset['test']) < num_examples:\n",
    "    num_examples = len(dataset['test'])\n",
    "\n",
    "for i in range(num_examples):\n",
    "    q = dataset['test'][i][\"question\"]\n",
    "    ref = dataset['test'][i][\"answer\"]\n",
    "    prompt = f\"Question: {q}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(loaded_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    ans = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\", 1).strip()\n",
    "    print(\"\\n--- Example\", i+1, \"---\")\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Reference:\\n\", ref)\n",
    "    print(\"Generated:\\n\", ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Example 1 --\n",
      "Question: How to concatenate string variables in Bash\n",
      "Reference: foo=\"Hello\"\n",
      "foo=\"${foo} World\"\n",
      "echo \"${foo}\"\n",
      "> Hello World\n",
      "\n",
      "\n",
      "In general to concatenate two variables you can just write them one after another:\n",
      "\n",
      "a='Hello'\n",
      "b='World'\n",
      "c=\"${a} ${b}\"\n",
      "echo \"${c}\"\n",
      "> Hello World\n",
      "Generated: $ cat /etc/bash_completion/bash_completion.sh\n",
      "$ cat /etc/bash_completion/bash_completion.sh\n",
      "$ cat /etc/bash_completion/bash_completion.sh\n",
      "$ cat /etc/bash_completion/bash_completion.sh\n",
      "$ cat /etc/bash_completion/bash_completion.sh\n",
      "$ cat /etc/bash_completion/bash_completion.sh\n",
      "$ cat /etc/bash_completion/bash_completion.sh\n",
      "$ cat /etc/bash_com\n",
      "\n",
      "\n",
      "-- Example 2 --\n",
      "Question: How do I iterate over a range of numbers defined by variables in Bash?\n",
      "Reference: for i in $(seq 1 $END); do echo $i; done\n",
      "\n",
      "edit: I prefer seq over the other methods because I can actually remember it ;)\n",
      "Generated: If you want to iterate over a range of numbers, you can do it like this:\n",
      "for i in range(1,10):\n",
      "    for j in range(1,10):\n",
      "        print(i,j)\n",
      "\n",
      "A:\n",
      "\n",
      "If you want to iterate over a range of numbers, you can do it like this:\n",
      "for i in range(1,10):\n",
      "    for j in range(1,10):\n",
      "        print(i,j)\n",
      "\n",
      "\n",
      "-- Example 2 --\n",
      "Question: How do I iterate over a range of numbers defined by variables in Bash?\n",
      "Reference: for i in $(seq 1 $END); do echo $i; done\n",
      "\n",
      "edit: I prefer seq over the other methods because I can actually remember it ;)\n",
      "Generated: If you want to iterate over a range of numbers, you can do it like this:\n",
      "for i in range(1,10):\n",
      "    for j in range(1,10):\n",
      "        print(i,j)\n",
      "\n",
      "A:\n",
      "\n",
      "If you want to iterate over a range of numbers, you can do it like this:\n",
      "for i in range(1,10):\n",
      "    for j in range(1,10):\n",
      "        print(i,j)\n",
      "\n",
      "\n",
      "-- Example 3 --\n",
      "Question: Git is not working after macOS update (&quot;xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools&quot;)\n",
      "Reference: The problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n",
      "\n",
      "Did not run into this on Sonoma.\n",
      "\n",
      "Maybe Apple fixed the process?\n",
      "\n",
      "\n",
      "Updated for Ventura\n",
      "\n",
      "After opening the terminal after restarting, I tried to go to my code, and do a git status, and I got an error and prompt for command line software agreement:\n",
      "So press space until you get to the [agree, print, cancel] optio\n",
      "Generated: It's not working after macOS update.\n",
      "\n",
      "A:\n",
      "\n",
      "The problem is that you are trying to run a command that is not in the developer path.\n",
      "You can try to run a command that is in the developer path:\n",
      "git remote add origin git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@githu...\n",
      "\n",
      "\n",
      "-- Example 3 --\n",
      "Question: Git is not working after macOS update (&quot;xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools&quot;)\n",
      "Reference: The problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n",
      "\n",
      "Did not run into this on Sonoma.\n",
      "\n",
      "Maybe Apple fixed the process?\n",
      "\n",
      "\n",
      "Updated for Ventura\n",
      "\n",
      "After opening the terminal after restarting, I tried to go to my code, and do a git status, and I got an error and prompt for command line software agreement:\n",
      "So press space until you get to the [agree, print, cancel] optio\n",
      "Generated: It's not working after macOS update.\n",
      "\n",
      "A:\n",
      "\n",
      "The problem is that you are trying to run a command that is not in the developer path.\n",
      "You can try to run a command that is in the developer path:\n",
      "git remote add origin git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@github.com:git@githu...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_examples = 3\n",
    "for i in range(num_examples):\n",
    "    q = dataset['test'][i][\"question\"]\n",
    "    ref = dataset['test'][i][\"answer\"]\n",
    "    prompt = f\"Question: {q}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(loaded_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    ans = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\", 1).strip()\n",
    "    ans_trim = (ans[:400] + '...') if len(ans) > 400 else ans\n",
    "    print(f\"\\n-- Example {i+1} --\\nQuestion: {q}\\nReference: {ref[:400]}\\nGenerated: {ans_trim}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a610ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "c:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\transformers\\data\\data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "c:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\transformers\\data\\data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10/200 2:18:36 < 54:51:57, 0.00 it/s, Epoch 0.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m trainer.train_dataset = train_dataset\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m train_info = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, train_info)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Save model and adapter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2852\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.args.max_steps = 50\n",
    "trainer.args.num_train_epochs = 1\n",
    "trainer.args.logging_steps = 10\n",
    "trainer.args.save_strategy = 'steps'\n",
    "trainer.args.save_steps = 25\n",
    "\n",
    "trainer.train_dataset = train_dataset\n",
    "\n",
    "train_info = trainer.train()\n",
    "print(\"Training finished:\\n\", train_info)\n",
    "\n",
    "trainer.save_model(trainer.args.output_dir)\n",
    "try:\n",
    "    model.save_pretrained(trainer.args.output_dir)\n",
    "    print('model.save_pretrained() success')\n",
    "except Exception as e:\n",
    "    print('model.save_pretrained() failed:', e)\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(trainer.args.output_dir)\n",
    "    print('PeftModel.save_pretrained() success')\n",
    "except Exception as e:\n",
    "    print('PeftModel.save_pretrained() failed:', e)\n",
    "\n",
    "import os\n",
    "print('lora-gptneo contents after training:', os.listdir('lora-gptneo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a6f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.args.output_dir: ./lora-gptneo\n",
      "trainer.args.save_strategy: SaveStrategy.EPOCH\n",
      "trainer.args.save_steps: 500\n",
      "trainer.state.global_step: 10\n",
      "trainer.state.max_steps: 10\n",
      "trainer.state.epoch: 1.0\n",
      "is_world_process_zero: True\n",
      "\n",
      "CWD: c:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\n",
      "lora-gptneo exists: True\n",
      "lora-gptneo contents before save: ['checkpoint-10']\n",
      "\n",
      "Saving model and adapter now to ./lora-gptneo\n",
      "model.save_pretrained() success\n",
      "model.save_pretrained() success\n",
      "PeftModel.save_pretrained() success\n",
      "lora-gptneo contents after save: ['adapter_config.json', 'adapter_model.safetensors', 'checkpoint-10', 'merges.txt', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n",
      "PeftModel.save_pretrained() success\n",
      "lora-gptneo contents after save: ['adapter_config.json', 'adapter_model.safetensors', 'checkpoint-10', 'merges.txt', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.args.output_dir: ./lora-gptneo\n",
      "trainer.args.save_strategy: steps\n",
      "trainer.args.save_steps: 100\n",
      "trainer.state.global_step: 9\n",
      "trainer.state.max_steps: 200\n",
      "trainer.state.epoch: 0.10714285714285714\n",
      "is_world_process_zero: True\n",
      "\n",
      "CWD: c:\\Users\\KIIT0001\\Desktop\\StackoverflowQ&A\n",
      "lora-gptneo exists: True\n",
      "lora-gptneo contents before save: ['adapter_config.json', 'adapter_model.safetensors', 'checkpoint-10', 'config.json', 'merges.txt', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n",
      "\n",
      "Saving model and adapter now to ./lora-gptneo\n",
      "model.save_pretrained() success\n",
      "model.save_pretrained() success\n",
      "PeftModel.save_pretrained() success\n",
      "lora-gptneo contents after save: ['adapter_config.json', 'adapter_model.safetensors', 'checkpoint-10', 'config.json', 'merges.txt', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n",
      "PeftModel.save_pretrained() success\n",
      "lora-gptneo contents after save: ['adapter_config.json', 'adapter_model.safetensors', 'checkpoint-10', 'config.json', 'merges.txt', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('trainer.args.output_dir:', trainer.args.output_dir)\n",
    "print('trainer.args.save_strategy:', trainer.args.save_strategy)\n",
    "print('trainer.args.save_steps:', getattr(trainer.args, 'save_steps', None))\n",
    "print('trainer.state.global_step:', trainer.state.global_step)\n",
    "print('trainer.state.max_steps:', trainer.state.max_steps)\n",
    "print('trainer.state.epoch:', trainer.state.epoch)\n",
    "print('is_world_process_zero:', trainer.is_world_process_zero())\n",
    "\n",
    "print('\\nCWD:', os.getcwd())\n",
    "print('lora-gptneo exists:', os.path.exists('lora-gptneo'))\n",
    "print('lora-gptneo contents before save:', os.listdir('lora-gptneo'))\n",
    "\n",
    "# Force save the adapter + trainer state\n",
    "print('\\nSaving model and adapter now to', trainer.args.output_dir)\n",
    "trainer.save_model(trainer.args.output_dir)\n",
    "try:\n",
    "    model.save_pretrained(trainer.args.output_dir)\n",
    "    print('model.save_pretrained() success')\n",
    "except Exception as e:\n",
    "    print('model.save_pretrained() failed:', e)\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(trainer.args.output_dir)\n",
    "    print('PeftModel.save_pretrained() success')\n",
    "except Exception as e:\n",
    "    print('PeftModel.save_pretrained() failed:', e)\n",
    "\n",
    "print('lora-gptneo contents after save:', os.listdir('lora-gptneo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel.save_pretrained: success\n",
      "base_model.config.save_pretrained: success\n",
      "lora-gptneo contents: ['adapter_config.json', 'adapter_model.safetensors', 'checkpoint-10', 'config.json', 'merges.txt', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('lora-gptneo', exist_ok=True)\n",
    "\n",
    "try:\n",
    "    model.save_pretrained('lora-gptneo')\n",
    "    print('PeftModel.save_pretrained: success')\n",
    "except Exception as e:\n",
    "    print('PeftModel.save_pretrained: failed ->', e)\n",
    "\n",
    "try:\n",
    "    base_model.config.save_pretrained('lora-gptneo')\n",
    "    print('base_model.config.save_pretrained: success')\n",
    "except Exception as e:\n",
    "    print('base_model.config.save_pretrained: failed ->', e)\n",
    "\n",
    "print('lora-gptneo contents:', os.listdir('lora-gptneo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9cd72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT children: ['.venv', 'logs', 'lora-gptneo']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk('.', topdown=True):\n",
    "\n",
    "    if root == '.':\n",
    "        print('ROOT children:', dirs)\n",
    "    if root.startswith('./lora-gptneo') and root.count(os.sep) <= 2:\n",
    "        print('LORA tree:', root, dirs, files)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
